{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa8d8652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'classification'],\n",
       "        num_rows: 149049\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'classification'],\n",
       "        num_rows: 16556\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"magnea/fake-news-formated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9d8f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'label'],\n",
       "        num_rows: 114522\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'label'],\n",
       "        num_rows: 12728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= dataset.filter(lambda d: d['title'] != None and d['content'] != None)\n",
    "dataset = dataset.rename_column('classification', 'label')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e8f822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9333d108d69848aebae6a2f904a938aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/114522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "381b1337fcc448b59eb5d679a74e0412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined'],\n",
       "        num_rows: 114522\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined'],\n",
       "        num_rows: 12728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label={0: 'fake', 1: 'real'}\n",
    "label2id={'fake': 0, 'real': 1}\n",
    "\n",
    "def combine_fields(d):\n",
    "\treturn {'combined': d['title'] + '\\n\\n' + d['content']}\n",
    "\n",
    "def transform_label(d):\n",
    "    d['label'] = label2id[d['label']]\n",
    "    return d\n",
    "\n",
    "splits = dataset.keys()\n",
    "for split in splits:\n",
    "    dataset[split] = dataset[split].map(combine_fields)\n",
    "    dataset[split] = dataset[split].map(transform_label)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a56ffcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e52659cd236e805',\n",
       " 'dataset_id': 1.0,\n",
       " 'title': \"South Africa's Dlamini-Zuma, ANC leadership contender, to become MP\",\n",
       " 'content': 'JOHANNESBURG (Reuters) - South African veteran politician and anti-apartheid activist Nkosazana Dlamini-Zuma, a leading contender to take over as head of the ruling ANC in December, will be sworn in as a member of parliament next week, a senior party official said on Friday. Dlamini-Zuma, the ex-wife of current ANC leader and South African President Jacob Zuma, does not hold a top position and could use a seat in parliament to raise her profile ahead of the party s December leadership conference.   She is going to be sworn in,  ANC Secretary General Gwede Mantashe was quoted as saying by the local EWN news network. The former health and foreign affairs minister s main opponent in the ANC leadership race is expected to be Deputy President Cyril Ramaphosa, a trade unionist-turned-business tycoon whom many investors would prefer to see running a country with serious economic challenges. Dlamini-Zuma is pushing for a more radical redistribution of wealth from whites to blacks, a policy that appeals to many poor people who resent the stark racial inequality that persists 23 years after the end of apartheid. The next head of the ANC will be the party s presidential candidate in 2019 general elections. ',\n",
       " 'label': 1,\n",
       " 'combined': \"South Africa's Dlamini-Zuma, ANC leadership contender, to become MP\\n\\nJOHANNESBURG (Reuters) - South African veteran politician and anti-apartheid activist Nkosazana Dlamini-Zuma, a leading contender to take over as head of the ruling ANC in December, will be sworn in as a member of parliament next week, a senior party official said on Friday. Dlamini-Zuma, the ex-wife of current ANC leader and South African President Jacob Zuma, does not hold a top position and could use a seat in parliament to raise her profile ahead of the party s December leadership conference.   She is going to be sworn in,  ANC Secretary General Gwede Mantashe was quoted as saying by the local EWN news network. The former health and foreign affairs minister s main opponent in the ANC leadership race is expected to be Deputy President Cyril Ramaphosa, a trade unionist-turned-business tycoon whom many investors would prefer to see running a country with serious economic challenges. Dlamini-Zuma is pushing for a more radical redistribution of wealth from whites to blacks, a policy that appeals to many poor people who resent the stark racial inequality that persists 23 years after the end of apartheid. The next head of the ANC will be the party s presidential candidate in 2019 general elections. \"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][4711]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b24661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7c04364b83404684ace9fa47f92492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/114522 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45bc328867947c78d5dfad2804e2103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 114522\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 12728\n",
       " })}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_data(d):\n",
    "    return tokenizer(d['combined'], padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "tokenized = {}\n",
    "for split in splits:\n",
    "    tokenized[split] = dataset[split].map(preprocess_data, batched=True)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13c3e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 12728\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edf81588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0749c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23297/2265083618.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='199' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [199/199 02:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5774004459381104,\n",
       " 'eval_model_preparation_time': 0.0007,\n",
       " 'eval_accuracy': 0.494343180389692,\n",
       " 'eval_runtime': 175.1342,\n",
       " 'eval_samples_per_second': 72.676,\n",
       " 'eval_steps_per_second': 1.136}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./benchmark\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True\n",
    "    ),\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e8c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nd608-generative-ai-project-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
