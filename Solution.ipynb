{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d19374e",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project\n",
    "\n",
    "In this notebook I am goint to apply parameter-efficient fine-tuning (PEFT) to boost the accuracy of detecting fake news from the [magnea/fake-news-formated](https://huggingface.co/datasets/magnea/fake-news-formated) dataset from HuggingFaceðŸ¤—.\n",
    "\n",
    "This dataset contains ~165k examples of news articles and a classification wether these news are legit or fake.\n",
    "\n",
    "For fine-tuning I am going to apply **low-rank adaptation (LoRA)** to the **GPT-2** model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d01ae33",
   "metadata": {},
   "source": [
    "### Dataset and Preprocessing\n",
    "\n",
    "I can see that the dataset is split about 90/10 into *training* and *test* data and that the news articles are seperated into title and content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8d8652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'classification'],\n",
       "        num_rows: 149049\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'classification'],\n",
       "        num_rows: 16556\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"magnea/fake-news-formated\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331838de",
   "metadata": {},
   "source": [
    "For consistency I want to sort out all rows where only one of the parts is available. Additionally I am going to rename the *classification* column to *label* in order to comply with the huggingface Trainer API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae9d8f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'label'],\n",
       "        num_rows: 114522\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'label'],\n",
       "        num_rows: 12728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= dataset.filter(lambda d: d['title'] != None and d['content'] != None)\n",
    "dataset = dataset.rename_column('classification', 'label')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fd042c",
   "metadata": {},
   "source": [
    "Next I am going to combine the *title* and *content* column into one in order to pass both in a standardized format into the model. Furthermore I am also mapping the labels to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90e8f822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined'],\n",
       "        num_rows: 114522\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined'],\n",
       "        num_rows: 12728\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label={0: 'fake', 1: 'real'}\n",
    "label2id={'fake': 0, 'real': 1}\n",
    "\n",
    "def combine_fields(d):\n",
    "\treturn {'combined': d['title'] + '\\n\\n' + d['content']}\n",
    "\n",
    "def transform_label(d):\n",
    "    d['label'] = label2id[d['label']]\n",
    "    return d\n",
    "\n",
    "splits = dataset.keys()\n",
    "for split in splits:\n",
    "    dataset[split] = dataset[split].map(combine_fields)\n",
    "    dataset[split] = dataset[split].map(transform_label)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a56ffcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e52659cd236e805',\n",
       " 'dataset_id': 1.0,\n",
       " 'title': \"South Africa's Dlamini-Zuma, ANC leadership contender, to become MP\",\n",
       " 'content': 'JOHANNESBURG (Reuters) - South African veteran politician and anti-apartheid activist Nkosazana Dlamini-Zuma, a leading contender to take over as head of the ruling ANC in December, will be sworn in as a member of parliament next week, a senior party official said on Friday. Dlamini-Zuma, the ex-wife of current ANC leader and South African President Jacob Zuma, does not hold a top position and could use a seat in parliament to raise her profile ahead of the party s December leadership conference.   She is going to be sworn in,  ANC Secretary General Gwede Mantashe was quoted as saying by the local EWN news network. The former health and foreign affairs minister s main opponent in the ANC leadership race is expected to be Deputy President Cyril Ramaphosa, a trade unionist-turned-business tycoon whom many investors would prefer to see running a country with serious economic challenges. Dlamini-Zuma is pushing for a more radical redistribution of wealth from whites to blacks, a policy that appeals to many poor people who resent the stark racial inequality that persists 23 years after the end of apartheid. The next head of the ANC will be the party s presidential candidate in 2019 general elections. ',\n",
       " 'label': 1,\n",
       " 'combined': \"South Africa's Dlamini-Zuma, ANC leadership contender, to become MP\\n\\nJOHANNESBURG (Reuters) - South African veteran politician and anti-apartheid activist Nkosazana Dlamini-Zuma, a leading contender to take over as head of the ruling ANC in December, will be sworn in as a member of parliament next week, a senior party official said on Friday. Dlamini-Zuma, the ex-wife of current ANC leader and South African President Jacob Zuma, does not hold a top position and could use a seat in parliament to raise her profile ahead of the party s December leadership conference.   She is going to be sworn in,  ANC Secretary General Gwede Mantashe was quoted as saying by the local EWN news network. The former health and foreign affairs minister s main opponent in the ANC leadership race is expected to be Deputy President Cyril Ramaphosa, a trade unionist-turned-business tycoon whom many investors would prefer to see running a country with serious economic challenges. Dlamini-Zuma is pushing for a more radical redistribution of wealth from whites to blacks, a policy that appeals to many poor people who resent the stark racial inequality that persists 23 years after the end of apartheid. The next head of the ANC will be the party s presidential candidate in 2019 general elections. \"}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][4711]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53812615",
   "metadata": {},
   "source": [
    "Finally I am using the `AutoTokenizer` to tokenize the dataset for the GPT-2 model. I am using padding and truncation to allign all rows in order to allows batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03b24661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 114522\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 12728\n",
       " })}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def preprocess_data(d):\n",
    "    return tokenizer(d['combined'], padding='max_length', truncation=True)\n",
    "\n",
    "\n",
    "tokenized = {}\n",
    "for split in splits:\n",
    "    tokenized[split] = dataset[split].map(preprocess_data, batched=True)\n",
    "\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13c3e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'dataset_id', 'title', 'content', 'label', 'combined', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 12728\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f2d05",
   "metadata": {},
   "source": [
    "# Evaluationg the Foundation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e77936",
   "metadata": {},
   "source": [
    "I am using the `AutoModelForSequenceClassification` to load the GPT-2 model extended with a classification head for 2 classes and set `requires_grad=False` for all model parameters as we do not want to alter the model during evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edf81588",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0743db",
   "metadata": {},
   "source": [
    "Finally I am using the `Trainer` class to evaluate the model performance on the *test* split of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea0749c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23297/2265083618.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='199' max='199' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [199/199 02:54]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.5774004459381104,\n",
       " 'eval_model_preparation_time': 0.0007,\n",
       " 'eval_accuracy': 0.494343180389692,\n",
       " 'eval_runtime': 175.1342,\n",
       " 'eval_samples_per_second': 72.676,\n",
       " 'eval_steps_per_second': 1.136}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./benchmark\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=64,\n",
    "        per_device_eval_batch_size=64,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True\n",
    "    ),\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9dd892",
   "metadata": {},
   "source": [
    "The `'eval_accuracy': 0.494343180389692,` shows that the model is pretty much guessing the class resulting in a 50/50 chance to be right. So at the moment we can als flip a coin at much lower operational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a1649",
   "metadata": {},
   "source": [
    "# Applying Low-Rank Adaptation (LoRA)\n",
    "\n",
    "The schema below demonstrates how LoRA reduces the number of parameters that need to be adjusted.\n",
    "\n",
    "![Schema of Low-Rank Adaptation(LoRA)](images/lora.png)\n",
    "\n",
    "First I am creating a configuration for my LoRA application which I will explain below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3e8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.01,\n",
    "    bias='none',\n",
    "    task_type='SEQ_CLS',\n",
    "    target_modules=['c_attn', 'c_proj'],\n",
    "    modules_to_save=['score']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0c5454",
   "metadata": {},
   "source": [
    "`r` is the rank of the update matricies A and B and `lora_alpha` defines the scaling. These parameters define the scaling of the resulting adaptation matrix as \n",
    "\n",
    "![lora scaling formula](images/scaling.png)\n",
    "\n",
    "with `r=4` and `lora_alpha=32` the scaling I picked is ``32/4 => 8``\n",
    "\n",
    "`bias='none'` means that the biases of the model remain untouched\n",
    "\n",
    "`task_type='SEQ_CLS'` tells the trainer that the models task is classification rather than generation\n",
    "\n",
    "`target_modules=['c_attn', 'c_proj']` restricts the adaptation to the attention modules which should be enough and saves time\n",
    "\n",
    "`modules_to_save=['score']` enables training of the classification head, if not set it remains the same\n",
    "\n",
    "`lora_dropout=0.01` adds a little bit of dropout in order to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bd5ce",
   "metadata": {},
   "source": [
    "The resulting model looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5f35c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 407,040 || all params: 124,848,384 || trainable%: 0.3260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thorsten/code/nd608-generative-ai-project-1/.venv/lib/python3.12/site-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=2304, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=768)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D(nf=3072, nx=768)\n",
       "              (c_proj): lora.Linear(\n",
       "                (base_layer): Conv1D(nf=768, nx=3072)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.01, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=4, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=4, out_features=768, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=768, out_features=2, bias=False)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model = get_peft_model(model=model, peft_config=config)\n",
    "lora_model.print_trainable_parameters()\n",
    "lora_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8dc37a",
   "metadata": {},
   "source": [
    "### Running Fine-Tuning\n",
    "\n",
    "In the following I am running 10 epochs of fine-tuning by applying LoRA. In order to save time I am only picking 128 random examples per epoch. Otherwise a full epoch would take about 55hrs on my RTX 5070ti (16GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c1788a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77704/1966471401.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='80' max='80' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [80/80 2:05:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.687464</td>\n",
       "      <td>0.613294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.781059</td>\n",
       "      <td>0.625236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.682606</td>\n",
       "      <td>0.627121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.808708</td>\n",
       "      <td>0.668290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.823840</td>\n",
       "      <td>0.656662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.119632</td>\n",
       "      <td>0.664048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.151145</td>\n",
       "      <td>0.667112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.147137</td>\n",
       "      <td>0.677718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.319463</td>\n",
       "      <td>0.668133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.363969</td>\n",
       "      <td>0.673397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6826058030128479,\n",
       " 'eval_accuracy': 0.6271213073538655,\n",
       " 'eval_runtime': 656.5363,\n",
       " 'eval_samples_per_second': 19.387,\n",
       " 'eval_steps_per_second': 1.212,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": (predictions == labels).mean()}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./lora-model\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        num_train_epochs=10,\n",
    "        weight_decay=0.01,\n",
    "    ),\n",
    "    train_dataset=tokenized[\"train\"].shuffle(seed=42).select(range(128)),\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ce259",
   "metadata": {},
   "source": [
    "After training I am evaluation the fine-tuned model on the same `test` dataset split as for the foundation model benchmark. As a result we can see that the accuracy has improved by 13 points to about ~63% which is already better than guessing after only 2 hours of training on very few samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e3d4ea",
   "metadata": {},
   "source": [
    "### Saving the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "815cab6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.save_pretrained(\"peft-fake-news\", save_adapter=True, save_config=True)\n",
    "lora_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480f9035",
   "metadata": {},
   "source": [
    "### Exploring results\n",
    "\n",
    "For exploration I am going to pick 20 random rows from the `test` dataset split and compare the prediction with the actual classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "765f5f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>combined</th>\n",
       "      <th>label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BREAKING: MASSIVE CYBER ATTACK ON ALL FEDERAL ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coronavirus May Mean Lights Out For Summer Cam...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deadly Somalia blast reveals flaws in intellig...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Head of Justice National Security Division to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No link between hypertension drugs and COVID-1...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HEREâ€™S THE LIST OF People We Elected Who Just ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Brothers ID'd as suicide bombers in Belgium\\n\\...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Puerto Rico evacuates area near crumbling dam,...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Trump win, Democratic setbacks cloud Pelosi's ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trump Confidante Confirms: The Donald Doesnâ€™t...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>For Some Arctic Plants, Spring Arrives Almost ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Virginia high court hears Republican voting-ri...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ted Cruz Was Snubbed For A Job In The Dubya Ad...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Barack Obamaâ€™s Final Arms-Export Totals Double...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rohingya grieve after baby dies in border cros...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Egypt's Sisi says spoke to Donald Trump by tel...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Trump, Malaysia's Najib skirt round U.S. probe...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Rewriting the Past\\n\\nLee Carroll Kryon â€“ Gree...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>OUTRAGE OVER BARE CHESTED GAYS COMPARING GAY M...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Irving Fields, Composer Who Infused Songs With...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             combined  label  predicted_label\n",
       "0   BREAKING: MASSIVE CYBER ATTACK ON ALL FEDERAL ...      1                1\n",
       "1   Coronavirus May Mean Lights Out For Summer Cam...      1                0\n",
       "2   Deadly Somalia blast reveals flaws in intellig...      0                0\n",
       "3   Head of Justice National Security Division to ...      1                0\n",
       "4   No link between hypertension drugs and COVID-1...      1                1\n",
       "5   HEREâ€™S THE LIST OF People We Elected Who Just ...      0                1\n",
       "6   Brothers ID'd as suicide bombers in Belgium\\n\\...      1                1\n",
       "7   Puerto Rico evacuates area near crumbling dam,...      1                1\n",
       "8   Trump win, Democratic setbacks cloud Pelosi's ...      1                1\n",
       "9    Trump Confidante Confirms: The Donald Doesnâ€™t...      0                1\n",
       "10  For Some Arctic Plants, Spring Arrives Almost ...      0                1\n",
       "11  Virginia high court hears Republican voting-ri...      1                1\n",
       "12  Ted Cruz Was Snubbed For A Job In The Dubya Ad...      1                1\n",
       "13  Barack Obamaâ€™s Final Arms-Export Totals Double...      1                1\n",
       "14  Rohingya grieve after baby dies in border cros...      0                0\n",
       "15  Egypt's Sisi says spoke to Donald Trump by tel...      1                1\n",
       "16  Trump, Malaysia's Najib skirt round U.S. probe...      0                1\n",
       "17  Rewriting the Past\\n\\nLee Carroll Kryon â€“ Gree...      0                1\n",
       "18  OUTRAGE OVER BARE CHESTED GAYS COMPARING GAY M...      1                1\n",
       "19  Irving Fields, Composer Who Infused Songs With...      0                1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "examples = tokenized[\"test\"].shuffle(seed=42).select(range(20))\n",
    "df = pd.DataFrame(examples)\n",
    "df = df[[\"combined\", \"label\"]]\n",
    "predictions = trainer.predict(examples)\n",
    "df[\"predicted_label\"] = np.argmax(predictions[0], axis=1)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2cf620e",
   "metadata": {},
   "source": [
    "# Inference\n",
    "\n",
    "I am going to test inference by loading the saved model and predicting a label for this [article from CBS News](https://www.cbsnews.com/news/white-house-ballroom-east-wing-demolition/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dba8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"White House begins demolition of part of East Wing for Trump's ballroom\"\n",
    "content = \"\"\"The Washington Post first reported on the demolition and published an image of the work. And on Monday, a pool reporter captured video of part of the East Wing being torn down. \n",
    "\n",
    "During an event Monday with the Louisiana State University baseball team at the White House, the president remarked on the construction, which he said \"just started today.\"\n",
    "\n",
    "\"You know we're building â€” right behind us â€” we're building a ballroom,\" Mr. Trump said during the celebration of the 2025 NCAA champions in the White House East Room. He pointed out, \"Right on the other side, you have a lot of construction going on, which you might hear periodically.\"\n",
    "\n",
    "The president also referenced the construction during a meeting with GOP senators on the Rose Garden patio Tuesday. Mr. Trump overhauled the Rose Garden and covered the grass earlier this year to make it easier to host guests.\n",
    "\"\"\"\n",
    "\n",
    "cbs_news_article = title + '\\n\\n' + content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbeb1800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe52a92e7f614814ab72002a3f0c2f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63441441d6f5492eacc0fe7df88c5e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "saved_model = AutoPeftModelForSequenceClassification.from_pretrained(\"peft-fake-news\",  num_labels=2)\n",
    "\n",
    "infer_input = tokenizer(cbs_news_article, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    infer_output = saved_model(**infer_input)\n",
    "    logits = infer_output.logits\n",
    "\n",
    "probabilities = torch.nn.functional.softmax(logits, dim=-1)\n",
    "predicted_class = torch.argmax(probabilities, dim=-1).numpy()[0]\n",
    "print(\"Predicted class:\", id2label[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e1e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nd608-generative-ai-project-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
